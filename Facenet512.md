Open-Source Face Recognition Models Surpassing FaceNet512

FaceNet (2015) set a high standard for face recognition with 512-dimensional embeddings and about 99.63% accuracy on LFW ￼. However, several newer open-source models now achieve even higher accuracy, especially in one-shot/few-shot scenarios where embeddings are compared via distance metrics. These models generalize better to unseen identities and harder benchmarks (e.g. frontal-profile or cross-age verification) than FaceNet512. Below we highlight key models (ArcFace, CosFace, MagFace, AdaFace, etc.), their architectures, training data, and performance on standard benchmarks, along with available implementations.

ArcFace (ResNet100 Backbone, 512-D Embedding)

ArcFace is a state-of-the-art face recognition model using an additive angular margin loss to enhance discriminative power ￼. It employs a deep ResNet100 backbone (100-layer IR-ResNet, ~65 million parameters) and produces 512-D embeddings for each face. Training: ArcFace was trained on the MS1M-V2 dataset (~5.8M cleaned face images of 85K identities) ￼. Inference: It expects aligned 112×112 RGB face crops and outputs a unit-length 512-D feature vector ￼, which can be compared by cosine similarity or Euclidean distance. Performance: ArcFace achieves ~99.8% verification accuracy on LFW and ~98.3% on CFP-FP (frontal-profile) and AgeDB-30 benchmarks ￼, significantly surpassing FaceNet’s 99.63% on LFW ￼. It also yields high true acceptance rates at low false-alarm rates on challenging sets (e.g. ~95.7% TAR@FAR=1e-4 on IJB-C) ￼. Open Implementation: Pretrained ArcFace models are openly available (e.g. in the InsightFace library). For example, the published ResNet100 ArcFace model (trained on MS1M-V2) is provided in MXNet/PyTorch and ONNX formats, and is reported to reach 99.83% LFW accuracy ￼. These weights can be used off-the-shelf for one-shot face matching by extracting embeddings and using a distance threshold.

CosFace (Large-Margin Cosine Loss Model)

CosFace (2018) introduced a large margin cosine loss for face recognition, encouraging a fixed cosine margin between classes. It typically uses a similar backbone to ArcFace (ResNet-based; the original used a 64-layer network) and 512-D embeddings. Training Data: CosFace was also trained on MS-Celeb-1M data (MS1M) in the original work, with a margin parameter $m=0.35$ ￼. Performance: CosFace achieves very comparable results to ArcFace – around 99.8% on LFW ￼ and in the high 98% range on CFP-FP and AgeDB-30 ￼. For instance, one report shows CosFace reaching 99.78% on LFW, 98.26% on CFP-FP, and 98.17% on AgeDB-30 ￼, exceeding FaceNet’s accuracy. It slightly trails ArcFace on some benchmarks (ArcFace had a minor edge on LFW/CFP-FP), but CosFace can outperform ArcFace on others (e.g. CosFace showed slightly higher AgeDB accuracy in one evaluation ￼). Open Source: Implementations of CosFace are available (e.g. in InsightFace and various research codebases), and one can obtain pretrained models with a ResNet backbone and CosFace loss. These models similarly output 512-D embeddings, usable for one-shot verification by cosine distance. For example, using a ResNet100 with CosFace loss on MS1M-V2 yields 99.8% LFW accuracy ￼.

MagFace (ResNet100, Quality-Aware Embeddings)

MagFace (2021) builds upon ArcFace by making the embedding magnitude reflect face image quality ￼ ￼. It introduces a magnitude-based adaptive margin: high-quality images are pushed to have larger feature norms and tighter clusters, while low-quality images have a relaxed margin. Architecture: MagFace uses the same 512-D feature dimension and ResNet100 backbone as ArcFace (trained on MS1M-V2) ￼. Performance: It achieves state-of-the-art accuracy on multiple benchmarks, slightly exceeding ArcFace. Reported results show 99.83% on LFW, 98.46% on CFP-FP, and 98.17% on AgeDB-30 ￼ – the highest overall in a comparative study. MagFace thus slightly improved upon ArcFace’s verification rates on all those benchmarks ￼ ￼. It also improved low false-positive rate recognition: e.g. on IJB-C, MagFace obtained higher TAR at very low FAR than ArcFace (e.g. 94.33% vs 94.08% at FAR=1e-4) ￼. Generalization: By encoding image quality into the embedding, MagFace yields more stable representations for unseen faces, avoiding overconfidence on low-quality inputs ￼. Availability: MagFace is open-source (official PyTorch code on GitHub), and pretrained weights are publicly provided. Using the released ResNet100 MagFace model (MS1M-V2 trained), one can directly extract 512-D features; it is plug-and-play for face verification. For example, an open pretrained MagFace model achieves 99.83% LFW accuracy and top results on AgeDB and CFP-FP ￼. This demonstrates its effectiveness for one-shot face matching with improved handling of image quality.

AdaFace (Quality Adaptive Margin, 512-D)

AdaFace (CVPR 2022) is a quality-adaptive face recognition model that adjusts the classification margin per sample based on feature norm (which correlates with image quality) ￼ ￼. In essence, high-quality faces get a larger effective margin (emphasizing hard samples), whereas low-quality faces get a smaller margin (de-emphasizing very uncertain samples) ￼ ￼. This strategy helps the model learn robust embeddings that generalize to unseen identities under quality degradation. Architecture & Training: AdaFace has been implemented with popular backbones (ResNet18, ResNet50, ResNet100); the authors provide pretrained models up to ResNet100 on various datasets (CASIA-WebFace, VGGFace2, MS1M V2/V3, WebFace4M/12M) ￼. The embeddings are 512-D (same dimensionality as ArcFace for compatibility). Performance: AdaFace matches the state of the art on standard benchmarks while greatly improving on low-quality images. For example, an AdaFace ResNet100 trained on MS1M-V2 achieves 99.82% on LFW and 98.49% on CFP-FP ￼ – slightly above ArcFace’s accuracy on the challenging CFP-FP profile test. Its AgeDB-30 result (≈98.1% ￼) is on par with other top models. Notably, AdaFace “substantially outperforms all baselines” on low-quality benchmark sets ￼. In evaluations on the IJB series (which include far less controlled images), AdaFace had 3–4% higher verification rates than ArcFace/CosFace ￼, a large gain in one-shot settings where probe images may be noisy or low-res. Generalization: By dynamically relaxing the decision boundary for difficult inputs, AdaFace produces embeddings that remain discriminative even for unseen identities in poor conditions ￼. Open-Source Access: AdaFace’s code and pretrained weights are openly available (PyTorch). The authors provide models like ResNet100 trained on MS1M and WebFace4M – for instance, the MS1M-V2 ResNet100 model can be downloaded and used for inference (taking 112×112 BGR images as input) ￼ ￼. This makes AdaFace readily usable for one-shot face recognition by extracting features and comparing via cosine distance. Its superior verification accuracy on both high-quality and low-quality sets makes it very attractive for robust face recognition ￼.

Transformer-Based Face Recognition Models

Inspired by the success of transformers in vision, recent works explore ViT (Vision Transformer) backbones for face recognition. Pure transformer models require large training datasets but can achieve accuracy on par with or exceeding CNN-based models. For example, Zhong et al. (2021) trained a ViT-based face recognition model on MS1M, using a patch-based transformer with a CosFace loss. This Face Transformer achieved 99.83% on LFW, essentially matching ArcFace’s performance ￼, and high results on AgeDB-30 (~98.0%) ￼. (On the pose-extreme CFP-FP set it scored ~96.5-96.7%, slightly below ArcFace’s 96.9% on that set ￼.) With even larger data, transformers shine: a recent “Large Vision Face” (LVFace) model trained on the gigantic WebFace42M dataset surpassed CNN models on major benchmarks ￼ ￼. In fact, a ViT-huge model with appropriate loss achieved state-of-the-art results, winning the ICCV2021 face recognition challenge by “outperform[ing] all CNN-based methods” ￼ ￼. These transformer models produce embeddings (often 512-D or 768-D) that can be used in the same way for one-shot verification (distance in feature space). Available Implementations: The Face Transformer by Zhong et al. is open-sourced with pretrained weights on CASIA-WebFace and MS1M ￼ ￼, allowing researchers to experiment with ViT backbones. As of 2025, large-scale transformer FR models (like LVFace) are at the cutting edge; when their weights become public, they will offer new options with even higher accuracy. In summary, transformer-based models – given sufficient training data – have proven capable of matching or surpassing the accuracy of ArcFace/CosFace ￼ ￼, indicating a promising direction for one-shot face recognition with improved generalization.

Comparison of Model Performance on Standard Benchmarks

To summarize, the table below compares FaceNet512 with several open models on common face verification benchmarks (LFW for general accuracy, CFP-FP for cross-pose, and AgeDB-30 for cross-age verification):

Model (Backbone)	LFW Accuracy	CFP-FP Accuracy	AgeDB-30 Accuracy
FaceNet512 (Inc-ResNet V1)	99.63% ￼	– (baseline)	– (baseline)
ArcFace (ResNet100)	99.83% ￼	98.27% ￼	98.28% ￼
CosFace (ResNet50/100)	99.78% ￼	98.26% ￼	98.17% ￼
MagFace (ResNet100)	99.83% ￼	98.46% ￼	98.17% ￼
AdaFace (ResNet100)	99.82% ￼	98.49% ￼	98.05% ￼

All the above models are open-source with publicly available pretrained weights, making them readily usable for feature extraction and one-shot learning. They consistently outperform the original FaceNet on LFW and show strong generalization to more challenging datasets. In practical terms, using any of these models, one can extract 512-D face embeddings and compare them (via cosine similarity or Euclidean distance) to decide if two faces are the same person. The higher verification rates (see table) on LFW, CFP-FP, AgeDB, etc., indicate these models provide more accurate and robust representations for unseen identities than FaceNet512 ￼ ￼. Each model has slightly different properties – ArcFace and CosFace excel with their margin-based discriminative power, MagFace and AdaFace add mechanisms to handle image quality variation, and transformer-based models offer an alternative that scales with data – but all represent the state-of-the-art for face recognition in one-shot or few-shot scenarios.

Sources: Key performance figures were drawn from published evaluations and papers for each model ￼ ￼ ￼, and implementation details from official repositories ￼ ￼. Each model listed above has been rigorously benchmarked (LFW, CFP-FP, AgeDB-30, etc.) and open-source code or weights can be obtained from the respective project pages or community repositories. The improvements in accuracy and verification rates demonstrate the advancement over FaceNet512 in both accuracy and generalization to unseen faces.
