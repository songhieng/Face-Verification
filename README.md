Enhancing Face Recognition Accuracy in Large-Scale Real-Time Systems

Introduction

Implementing accurate face recognition on real-time camera feeds with a database of hundreds of thousands of identities is a challenging task. Several factors can degrade accuracy in unconstrained settings: faces can appear in varying poses, under different lighting or low-resolution conditions, and may be occluded (e.g. by masks or glasses). Such variability in pose, illumination, and occlusion is known to significantly challenge face recognition systems ￼. In recent years, deep learning advancements (improved network architectures, training losses, and massive training datasets) have greatly boosted face recognition accuracy even under difficult conditions ￼. However, achieving high accuracy at large scale and in real time requires careful selection of models and techniques. This report surveys strategies, techniques, and technologies to improve face recognition accuracy for large-scale, real-time systems. We focus on open-source solutions wherever possible, covering: improvements over the user’s current pipeline (FaceNet with 512-d embeddings and RetinaFace detector), methods to handle pose and occlusion, approaches for scaling to 100k+ identities, real-time optimizations, and recent advances like ArcFace, AdaFace, and transformer-based models.

Face Detection and Alignment

Accurate face detection is the foundation of a reliable recognition pipeline. RetinaFace (used in the current system) is a state-of-the-art one-stage detector that jointly predicts face boxes and 5 facial landmarks for alignment ￼. It achieves high accuracy on challenging benchmarks (e.g. ~91.4% AP on WIDER Face Hard set) and even boosts downstream recognition performance (e.g. improving ArcFace verification rates on IJB-C) ￼. RetinaFace’s multi-task learning (landmarks and even a 3D mesh branch) helps robustly localize faces across scales, and with a lightweight backbone it can run in real-time (even on a single CPU core for VGA images) ￼. To ensure maximum accuracy, it is important to use the predicted landmarks to perform alignment – e.g. affine transform the face crop to a canonical orientation (eyes horizontal, standardized scale) before embedding. This alignment reduces intra-person variation due to rotation and scale.

Improvements or alternatives to RetinaFace can further enhance speed or accuracy. One notable option is SCRFD, an efficient detector introduced in 2021. SCRFD uses a optimized search space for computation and sample distribution, achieving a superior accuracy-speed trade-off ￼. For example, an SCRFD model with ~34 GMACs (floating ops) outperformed a prior state-of-the-art detector by 4.78% AP on WIDER Face (hard set) while running 3× faster on GPU ￼. The InsightFace project provides open-source implementations of SCRFD ￼, making it a drop-in replacement for RetinaFace with variants tailored to different resource budgets (from very lightweight models to heavy but ultra-accurate ones). In practice, if real-time performance is paramount (e.g. many camera feeds or high FPS), a lighter model like RetinaFace-MobileNet0.25 or a small SCRFD can be used on edge devices, whereas more powerful GPUs can handle larger SCRFD models for maximum accuracy.

Face alignment remains important even with robust detectors. Aligning each detected face using the five landmark points (eyes, nose base, mouth corners) helps normalize pose to some extent. This can improve recognition by making the face orientation more consistent for the embedding network. Most modern detectors (RetinaFace, SCRFD) output these landmark coordinates, which can be used to perform an affine transform of the face patch. Open-source utilities (e.g. InsightFace Python API or dlib) can perform this alignment easily. By reducing in-plane rotation and scale differences, alignment makes the embeddings from profile vs frontal faces more comparable. In summary, using a high-quality face detector (RetinaFace/SCRFD) with proper alignment sets a strong foundation for downstream recognition.

Advanced Face Recognition Models and Training Techniques

The choice of face embedding model and its training method has a major impact on accuracy. The user’s current model, FaceNet (512-dimension), was groundbreaking in 2015 (using an Inception-ResNet v1 backbone trained with triplet loss) and achieved ~99.6% on LFW. However, several modern architectures and loss functions have since pushed accuracy even higher, especially for large-scale recognition. Table 1 provides a comparison of prominent face recognition models/approaches and their key innovations:

Model/Method	Year	Key Approach	Open-Source Availability	Notable Improvements
FaceNet ￼ ￼	2015	Inception-ResNet backbone; 128–512d embedding; trained with triplet loss and online hard negative mining.	Yes – Official TensorFlow model and many re-implementations.	First to use deep embedding + triplet loss at scale; 99.6% LFW accuracy.
ArcFace	2019	ResNet-100 backbone (typically); Additive Angular Margin (ArcFace) loss for classification training.	Yes – InsightFace provides pretrained models ￼.	Highly discriminative embeddings due to angular margin – outperforms previous losses on many benchmarks ￼. Code released for research.
CosFace (LMCL)	2018	ResNet backbone; Additive Cosine Margin loss (a precursor to ArcFace).	Yes – available in InsightFace, OpenFace, etc.	Introduced cosine margin to increase inter-class separation; improved over softmax baselines.
CurricularFace	2020	ResNet backbone; curriculum learning for margins – dynamic margin that emphasizes hard samples more as training progresses.	Yes – implementations on GitHub (e.g. DeepInsight).	Enhanced training by gradually focusing on harder samples, improving recognition especially on difficult sets.
MagFace	2021	ResNet backbone; Magnitude-based feature learning – encourages embedding norm to reflect face image quality ￼ ￼.	Yes – Official code on GitHub ￼.	Learns a quality-aware embedding: high-quality faces have larger feature norms. This helps filter out low-quality or noisy face images and improves robustness ￼.
AdaFace	2022	Any backbone; Adaptive margin in ArcFace loss based on image quality (estimated by feature norm) ￼.	Yes – Open source (CVLFace repo) with various backbones ￼ ￼.	State-of-the-art performance on low-quality face datasets by treating high-quality and low-quality samples differently during training ￼ ￼. Particularly improves verification on challenging sets (IJB-B/C, TinyFace) without sacrificing overall accuracy.
Attention-Based or Transformer models	2021–2023	Vision Transformer (ViT) backbones or hybrid CNN–ViT models for face recognition; some use part-based attention to facial regions ￼ ￼.	Partially – research code (e.g. part fViT in PyTorch) is available ￼, but not yet as widely adopted as CNN models.	Transformers can model global relations; Part ViT approaches extract patches around landmarks and feed a ViT, achieving new state-of-art on benchmarks ￼ ￼. Still require large training data; ongoing research into efficiency.

Table 1: Comparison of face recognition models/techniques and their innovations.

The ArcFace approach stands out as a widely adopted improvement over FaceNet. ArcFace introduced an additive angular margin to the softmax loss, which enforces a stricter distinction between identities in angular (cosine similarity) space ￼ ￼. This leads to highly discriminative embeddings on a hypersphere, significantly improving verification and identification accuracy across many benchmarks ￼. ArcFace’s original paper demonstrated state-of-the-art results on ten benchmarks, and importantly it provided an open-source implementation and pretrained models ￼. In practice, swapping out a FaceNet model for a pretrained ArcFace model (for example, a ResNet100 trained on MS1M or Glint360K with ArcFace loss) can yield a noticeable boost in accuracy, especially for large pose and age variations. Numerous open-source resources (the InsightFace library, DeepFace, FacePy, etc.) provide ArcFace models ready to use.

Building on ArcFace, AdaFace (CVPR 2022) is a recent advance addressing a key issue: not all face images are equal in quality. AdaFace adapts the margin during training based on an image’s quality, where quality is estimated by the feature norm ￼. Intuitively, if a face image is low-quality (blurry, low-res, occluded, etc.), the model uses a smaller effective margin (making it a bit “easier” for that image to be classified correctly), whereas high-quality images use a larger margin. This quality adaptive loss allows the model to learn from hard examples without overly penalizing them for being hard due to factors beyond identity ￼ ￼. AdaFace has been shown to improve face verification performance on challenging benchmarks like IJB-C and TinyFace over previous state-of-the-art models ￼. The authors have released code and even a new framework (CVLFace) supporting AdaFace with various backbones (including ViTs) ￼ ￼. For a system dealing with many low-quality surveillance frames, using an AdaFace-trained model (or fine-tuning an ArcFace model with the AdaFace loss) could improve accuracy on the harder cases while maintaining overall discriminative power.

Another approach, MagFace (CVPR 2021), explicitly learns an embedding space where the magnitude of the feature vector correlates with face image quality ￼. During training, MagFace pushes “easy” (high-quality) samples to have embeddings with larger norms and clusters them tightly, while “hard” samples (low-quality or ambiguous) end up with smaller-norm embeddings further from the class center ￼ ￼. In effect, the model self-assesses image quality – a low-quality face yields a low-norm embedding, indicating the recognition result might be less reliable. This can be useful at inference: one can set a threshold on feature norm to decide if a face image is of sufficient quality to attempt recognition ￼. MagFace improved recognition accuracy in the wild and provided a built-in face quality measure ￼. Both MagFace and AdaFace show the trend of incorporating image quality awareness into the training process, which is highly relevant for real-time video feeds where blur, lighting, or occlusion can vary widely.

Beyond loss functions, researchers have also explored architectural innovations for face recognition. While CNNs have been dominant, Vision Transformers (ViT) are being studied as alternatives or complements to CNN backbones. A vanilla ViT can achieve competitive accuracy if trained on large-scale face data with the right settings ￼. More interestingly, a part-based ViT approach (Sun et al., BMVC 2022) uses a lightweight CNN to predict facial landmark points, then extracts patches (eyes, nose, mouth regions, etc.) which are fed into a ViT for recognition ￼ ￼. This model, called part fViT, effectively focuses the attention on key facial parts and was shown to surpass a standard ViT, setting a new state-of-the-art on several face benchmarks ￼ ￼. The attention mechanism helps the model handle occlusions or pose – if one part of the face is obscured, other visible parts still contribute. Open-source code for such transformer-based models is emerging (e.g. the AdaFace authors’ CVLFace repository notes support for ViT and Swin-Transformer backbones ￼). While these transformer models are not yet as widely deployed as ArcFace-based CNNs, they represent a promising avenue, especially as computing hardware evolves. In practice, one might experiment with a state-of-the-art open model from the InsightFace Model Zoo (which now includes SE-ResNet, MobileFaceNet, and even ViT-based models) and evaluate accuracy gains over the older FaceNet512 network.

Handling Pose Variation and Occlusion

One of the toughest challenges for face recognition is pose variation – faces seen in profile or at odd angles can appear very different from frontal views of the same person. Extreme head poses reduce the overlap of facial features with the frontal view, confounding typical embeddings. To tackle this, multiple strategies can be employed concurrently:

Pose-Invariant Recognition Strategies

Example of face frontalization: profile faces (left of each pair) are transformed to a synthetic frontal view (right) using a GAN-based method ￼ ￼. Frontalization can improve recognition by restoring features lost in profile views, though care must be taken to preserve identity.

A direct approach to handle pose is face frontalization – generate a frontal or near-frontal view of the face before recognition. Earlier methods used 3D morphable models or warping techniques to normalize pose. More recent methods leverage GANs to synthesize photorealistic frontal faces from profiles. For instance, TP-GAN (ICCV 2017) and others can produce an identity-preserving frontal view given a single input image at various yaw angles ￼. As shown in the image above, a GAN can recover a plausible full-face appearance even when the input is a profile. By running the recognition model on the generated frontal image, accuracy can improve because the embedding network (often trained mostly on near-frontal faces) sees a more familiar view. Open-source implementations of frontalization GANs exist (e.g. the official TP-GAN project on GitHub provides code and examples). However, frontalization is not without challenges: GAN-generated images can introduce artifacts or distortions, and processing every face with a GAN may be too slow for real-time systems.

An alternative, introduced by Jung et al. (ArXiv 2025), is Pose-TTA (Test-Time Augmentation) for face recognition ￼ ￼. Instead of fully frontalizing a profile face (which risks losing some identity cues due to distortion), Pose-TTA generates additional augmented views of a face at multiple poses for comparison. In their approach, a pretrained “portrait animation” model is used at inference time: given an input face and a target pose (say, another viewpoint), it synthesizes the face in that new pose while maintaining the person’s identity ￼. For example, if the system must compare a probe face (profile) to a gallery of frontal faces, Pose-TTA would rotate the gallery faces to profile angles or rotate the probe face to frontal, so that both faces in each comparison share the same pose ￼. This avoids the difficult task of generating a perfect frontal image from a profile; instead it creates two comparably posed images. Combined with a weighted feature aggregation to account for any GAN-induced artifacts, this method consistently improved accuracy on diverse datasets without needing to retrain the recognition model ￼. The upside is that Pose-TTA can be bolted on top of an existing pipeline (it’s model-agnostic). The downside is the extra computation to generate and compare multiple augmentations per face. Depending on the real-time constraints, one might use a lighter-weight generator or only apply such augmentation for faces detected with extreme yaw angles. If extreme pose faces are a major failure case in the current system, implementing a form of this test-time augmentation could yield significant gains in identifying those profiles ￼.

At the training stage, another strategy is to explicitly incorporate pose diversity. Modern face datasets (like WebFace42M or CASIA-WebFace) include faces across many poses ￼, and models like ArcFace are inherently more robust to pose than older models that lacked such data. Some research goes further: pose-aware training where the model might learn pose embeddings or use a multi-task loss to predict pose in addition to identity. There are also loss functions that penalize pose-induced intra-class variance. For example, a head pose-adaptive margin approach (HeadPose-Softmax, 2022) dynamically adjusted the difficulty (margin) during training based on the pose of the face ￼ – easier for profile faces, harder for frontal – akin to AdaFace but specifically for pose. Such methods, if available in open-source frameworks, could be used to fine-tune a model to make it more pose-invariant.

Lastly, leveraging the video nature of camera feeds, one can use temporal information to mitigate pose issues. In a continuous video, a person who is initially in profile might eventually turn toward the camera. By tracking faces across frames (using a tracker or by face re-identification), the system can accumulate multiple observations of the same person. A simple implementation: maintain a short-term gallery of recent face embeddings for each tracked individual, which likely include a mix of angles. Recognition can be done on the set of embeddings rather than a single frame. Academic works on video face recognition adopt this concept – e.g. the Neural Aggregation Network (NAN) uses an attention mechanism to combine features from multiple video frames into one robust representation ￼ ￼. It automatically gives more weight to high-quality, frontal faces and down-weights blurred or profile faces in the aggregation ￼. In a real-time system, a simplified version could be to average the embeddings of a track or pick the “best” frame (highest face detection confidence or largest feature norm) to represent the person. This multi-frame voting or aggregation can significantly improve accuracy for difficult poses, at the cost of a slight delay (waiting a few frames to collect info). Since the user’s scenario involves live camera feeds, integrating a lightweight tracker and using short-term face feature aggregation is a practical way to boost accuracy under pose variations without heavy computation.

Occlusion and Illumination Robustness

Occlusions, such as face masks, sunglasses, or objects partially blocking the face, present another challenge. A face recognition system should ideally still recognize a person from their visible parts. Handling occlusion can be approached via both training and architectural design:
	•	Data augmentation for occlusion: Incorporating occluded faces in training can improve robustness. For example, during the COVID-19 pandemic, researchers improved face recognizers by augmenting training data with synthetic face masks or by fine-tuning models on datasets of masked faces. If one cannot retrain, one could maintain two models – one trained on full faces and another on masked faces – and use a mask detector to decide which model’s embedding to use. Open-source projects like insightface released a mask-augmented recognition model in 2020 (termed “Masked Face Recognition Challenge winner”). Ensuring the model has “seen” occlusions like sunglasses, hats, etc., in training will make it more tolerant when those occur in the wild.
	•	Attention mechanisms: Architecturally, attention can help the model focus on visible facial features while ignoring occluded regions. Some recent CNN variants include self-attention or external attention modules that learn to mask out irrelevant pixels. For example, a partial face recognition method (He et al. 2019) used spatial attention to specifically extract features from non-occluded patches of the face. Similarly, the earlier-mentioned part-based ViT inherently can pay attention to specific facial parts – if the mouth region is occluded (say by a mask), the transformer can still rely on eyes and nose patches to identify the person ￼ ￼. Designing models or using pre-trained ones that have such mechanisms will improve resilience to occlusion.
	•	Landmark or region-based matching: Another practical technique is to perform face recognition on facial regions separately. For instance, one could compute embeddings for the upper-half and lower-half of the face and compare those separately or fuse them. In cases of masked faces, the lower-half embedding would be unreliable, but the upper-half (eyes and forehead) might still match. There are algorithms (and a few open-source implementations) for “block-wise” face recognition that divide the feature map into regions.
	•	Image quality assessment: Occlusion often coincides with a drop in certain quality metrics (e.g. a face with a mask is missing half the information). Employing a face image quality estimator in the pipeline can be useful. Open-source toolkits like FIQA (Face Image Quality Assessment) use deep models to score how well a face image will likely match to others ￼ ￼. If a face’s quality is below a threshold (perhaps due to heavy occlusion or extreme lighting), the system might flag the match as low confidence, or avoid making a false positive by requiring higher similarity. AdaFace and MagFace implicitly provide a measure of quality via feature norm ￼ ￼ – this could be leveraged online (for example, if the feature norm is very low, treat the face as not confidently recognizable and perhaps defer decision or use a different strategy).

Illumination variations and poor lighting also affect accuracy. To counteract lighting issues:
	•	Photometric normalization: As a preprocessing step, applying normalization to face images can help. Classical techniques include converting to grayscale, histogram equalization, gamma correction, or more advanced normalization like the Tan–Triggs method (historically used in face recognition to handle shadows). These can make the input lighting more consistent with training data. However, modern deep models often can handle moderate lighting variation if trained on diverse data.
	•	Illumination augmentation in training: If one is training or fine-tuning a model, include random brightness, contrast, and color jitter augmentation. This forces the network to rely on invariant features. There are also 3D face reconstruction methods that can relight faces to a canonical illumination, but those are heavy to deploy in real time.
	•	Use infrared/thermal cameras: (If applicable) In some surveillance scenarios, combining RGB with IR imagery can help when visible light is low. But this is more about hardware setup than algorithms.

Ultimately, approaches like AdaFace and MagFace inherently tackle “low-quality” conditions – which include poor lighting – by either adapting margins or reflecting quality in the embedding ￼ ￼. By using a model trained with such techniques, the impact of illumination differences is reduced because the model has learned to treat low-illumination faces appropriately in the loss function (not overly tightening the decision boundary on them, for instance).

In summary, to improve accuracy under occlusion and lighting issues, one should combine preventative measures (data augmentation, perhaps specialized models for masked faces) with robust model choices (attention mechanisms, quality-aware losses) and possibly preprocessing (alignment, normalization) to mitigate these factors. These techniques ensure that even under suboptimal conditions, the system extracts the most useful features for recognition.

Real-Time Inference Optimization

Maintaining high accuracy is only part of the goal – the system must also run in real time. With live camera feeds (potentially multiple) and a large identity database, optimization is critical to meet time constraints. Key strategies include:
	•	Model efficiency and acceleration: Use architectures that balance accuracy and speed. For example, a large ResNet100 ArcFace model might be very accurate but could be overkill for real-time if the hardware is limited. Consider smaller but still powerful models like MobileFaceNet or EfficientNet-based embeddings for an excellent speed/accuracy trade-off. MobileFaceNet, for instance, has on the order of 4 million parameters and can run over 2× faster than a MobileNet-V2 on face verification tasks with only a slight drop in accuracy ￼. Researchers have proposed improved MobileFaceNet variants that maintain high accuracy (e.g., 99.2%+ on LFW) while using minimal FLOPs ￼ – these are well-suited for edge devices. If the highest accuracy is required, one can still use a heavy model on a GPU, but deploy optimizations like batching and asynchronous processing. For instance, if multiple faces are detected in one frame, running them through the embedding network as a batch is more efficient than one-by-one.
	•	Hardware acceleration: Leverage libraries and frameworks that optimize inference. NVIDIA’s TensorRT can take a trained model and optimize it (through layer fusion, precision reduction, etc.) for dramatically faster inference on GPU. Similarly, OpenVINO can optimize models for Intel CPUs and VPUs. Using FP16 (half-precision) or INT8 quantization can often double the throughput with only minor accuracy loss, especially if the model was trained with quantization-aware techniques. Many open-source face recognition models (ArcFace, MobileFaceNet, etc.) have been successfully converted to ONNX and then to TensorRT/OpenVINO with significant speedups. For example, an ArcFace ResNet50 model quantized to INT8 and accelerated via OpenVINO can run several times faster on CPU than the original FP32 model, with accuracy drop within ~0.5%. The user should profile their current pipeline and identify bottlenecks – detection and recognition CNNs usually dominate. Applying these optimizations can often bring a non-real-time pipeline into the real-time range.
	•	Pipeline parallelism and frame skipping: With multiple camera feeds, it’s important to design the system to handle them in parallel (e.g. one thread or process per camera, or a centralized frame queue processed by a pool of workers). Within a single feed, using a tracking approach as mentioned can reduce computation: you might not need to run the face detector on every single frame for every person. For example, detect faces and recognize them on frame 1, then track those faces for the next N frames (using a lightweight KCF or optical-flow tracker). As long as the track is consistent, you can assume the identity remains the same and only occasionally re-run the full detection+recognition (or if the track is lost/new face appears). This dramatically cuts down the number of CNN inferences per second per face, allowing the system to handle higher camera frame rates or more cameras. Many practical surveillance solutions do this to achieve real-time performance.
	•	Scalability with more compute: If the system must scale up (e.g., more cameras or higher resolution), consider a distributed or cloud-based setup where different GPUs handle different tasks. One possible design: have a dedicated GPU for face detection for all incoming streams (since detection is a common bottleneck), then crop faces and send them to a face recognition GPU that computes embeddings, and finally a CPU or another GPU that does the database search. Using message queues or shared memory can facilitate this pipeline. This way each component is optimized and can be scaled independently (e.g., multiple detection workers feeding into multiple recognition workers).

In summary, real-time optimization comes from choosing efficient models, using the right software optimizations (TensorRT/ONNX/OpenVINO), and designing the processing pipeline to avoid unnecessary repeated computation (through tracking and parallelism). By doing so, one can handle the heavy load of high-resolution video and large identity databases while still benefiting from the improved accuracy of advanced models.

Large-Scale Identity Retrieval and Scaling

Handling hundreds of thousands of identities presents challenges in both memory and search speed. A naive linear search through 100k embeddings for every face would actually not be terribly slow (100k dot-products can be ~0.1–0.5 milliseconds on a modern GPU), but as the database grows to millions, more sophisticated methods are needed. The key is to use an Approximate Nearest Neighbor (ANN) search library to index the embeddings for fast similarity lookups. Two leading open-source options are Faiss (from Facebook AI) and ScaNN (from Google Research).
	•	Faiss (Facebook AI Similarity Search): Faiss is a widely used C++/Python library optimized for efficient similarity search on dense vectors of any size ￼. It offers a variety of indexing methods covering a spectrum of speed/memory trade-offs ￼ ￼. For example, Faiss can perform brute-force L2 or inner-product search using BLAS libraries (with GPU acceleration) for smaller databases, or use advanced techniques like IVF (inverted file indices) with PQ (product quantization) for billion-scale data ￼ ￼. It’s optimized for both CPU and GPU, making it versatile for different deployment scenarios ￼. In the user’s case (~100k vectors), one could use a flat index (exact search) on GPU which would be extremely fast, or an IVF index that clusters the embeddings and only searches a portion of them for further scalability. Faiss’s state-of-the-art GPU implementation can search millions of vectors in milliseconds ￼. If memory is a concern, Faiss supports compressing vectors by quantization. For instance, one can compress 512-d float embeddings to 128 bytes each with minor accuracy loss, reducing memory by 4× and improving cache efficiency. Faiss is open-source (MIT license) and has a large community; integrating it into a Python pipeline (there are bindings) or C++ backend is straightforward.
	•	ScaNN (Scalable Nearest Neighbor Search): ScaNN is Google’s ANN library focused on maximum inner product search (which is suitable when using cosine similarity with normalized embeddings). It introduces an improved vector quantization technique (anisotropic quantization) that achieved about 2× higher throughput than other libraries in Google’s benchmarks ￼ ￼. ScaNN builds an index that prunes the search space efficiently and uses quantization to compute distances quickly ￼ ￼. For example, ScaNN might encode your 512-d embeddings into a smaller representation and use a combination of hashing and quantization to narrow down candidate matches. It’s also open-source and can be used via Python. In practice, whether Faiss or ScaNN is better may depend on your exact data and hardware – both are top-tier. ScaNN might shine if your similarity metric is inner-product and you want very low query latency with a slight accuracy trade-off.
	•	HNSW (Hierarchical Navigable Small World graphs): Another popular ANN approach is using graph-based indices. HNSWlib is an open-source library implementing this, and Faiss even offers HNSW as an index type. HNSW builds a navigable small-world graph of the vectors in multiple layers; query vectors traverse the graph to find nearest neighbors in sub-linear time. HNSW is known for high recall (even 100% recall with the right parameters) and excellent performance for up to tens of millions of points, though it can use more memory. It’s a great choice when one needs very high accuracy ANN. According to one comparison, both HNSWlib and ScaNN are among the fastest and most accurate libraries for vector search in large datasets ￼. The user could consider HNSW if the dataset grows or if exact recall is required; HNSWlib is also easy to integrate (pure C++ with Python bindings).

In practical terms, for ~100k identities, one could use Faiss with a flat (exact) index on GPU and get sub-millisecond search times, which is effectively real-time. For larger scales (e.g. 1 million or 10 million faces), an IVF+PQ index in Faiss or ScaNN’s quantized index would allow the system to handle searches in a few milliseconds while keeping memory usage reasonable ￼. The key is to benchmark and tune the index – these libraries typically allow choosing the number of clusters, the number of probes, etc., to balance speed vs accuracy. For instance, Faiss IVF might use 1024 clusters and search 20 of them per query: this might get ~99% of the exact search accuracy in a fraction of the time.

It is also important to consider scalability of updates and maintenance: if identities will be added or removed frequently, some ANN indices (like IVF) can be updated incrementally, whereas others may require rebuilding. Faiss does support adding vectors to an index; HNSW does as well (though removal is harder). If the application is more static (the gallery of known faces doesn’t change often), a highly optimized static index is fine. If it’s dynamic, one might also consider using a dedicated vector database like Milvus or ElasticSearch’s KNN plugin. These systems build on libraries like Faiss/HNSW under the hood but add distributed querying, persistence, and auto-indexing. Milvus (an open-source vector database) for example can manage sharding an index across nodes to handle beyond-million scales easily, and provides a simple interface to query by embedding.

One additional consideration for large-scale recognition is setting appropriate thresholds for matches. As the number of identities grows, the chance of a false positive match can increase if the threshold is not strict. With a good embedding (ArcFace or similar), distances between different people are usually large, but at 100k+ people, you may encounter some look-alikes. It’s wise to use a verification threshold that achieves a low false accept rate (FAR) at the operating point you need (e.g., FAR of 1e-4 or 1e-5). This can be determined from validation experiments or from known standards (ArcFace on certain benchmarks might say TAR=99% at FAR=1e-6, etc.). Also, using the feature norm (quality) as mentioned can aid here: require a minimum feature norm for a positive identification, to avoid low-quality input yielding a wrong but somewhat similar identity.

Finally, from a training scalability perspective, if one ever trains a model on a dataset with hundreds of thousands of identities (or more), there are techniques like Partial FC (used in InsightFace) which sample a subset of classes each iteration to reduce memory, while still effectively training on huge class counts ￼. This was noted in the AdaFace repository update, indicating that modern frameworks can handle very large identity classification problems efficiently ￼. This is more relevant to model training than inference, but it’s good to be aware that open-source implementations exist to train models on, say, 100k or 1M identities without needing an enormous GPU memory.

In conclusion, for large-scale retrieval, the recommendation is to integrate an ANN search library like Faiss or ScaNN for the identity database. These will ensure that lookup time remains negligible even as the database grows, and they come with flexibility to tune performance. Both libraries are open-source and battle-tested in industry for exactly this purpose. By pairing a strong embedding model (ArcFace/AdaFace, etc.) with an efficient ANN index, the system can achieve both accuracy and scalability.

Conclusion

Building a high-accuracy, large-scale face recognition system for real-time camera feeds requires a holistic approach: better models, smarter processing, and efficient infrastructure. On the model side, upgrading from FaceNet512 to modern embeddings like ArcFace (with an InsightFace model) or AdaFace can significantly improve discriminative power, especially for varied poses and difficult images ￼ ￼. Using robust detectors like RetinaFace or SCRFD with proper face alignment ensures the embeddings get good input ￼ ￼. To handle pose extremes, techniques such as face frontalization or Pose-TTA (test-time augmentations) and multi-frame aggregation can recover missing information and boost recognition rates for profile views ￼ ￼. Likewise, addressing occlusion and lighting through data augmentation, attention mechanisms, and quality-aware models (AdaFace, MagFace) makes the system more resilient to real-world conditions ￼ ￼. All these accuracy improvements are made practical by engineering the system for speed: leveraging model compression and acceleration (e.g. MobileFaceNets, TensorRT) and employing fast vector search (Faiss/ScaNN) to keep identity retrieval instantaneous ￼ ￼. By combining these strategies – state-of-the-art open-source algorithms with careful optimizations – the user can greatly enhance face recognition accuracy without sacrificing real-time performance or scalability. The result will be a system that robustly recognizes faces from live video, even under challenging poses and conditions, all while scaling to hundreds of thousands of identities in a responsive manner.

Sources: The information and techniques discussed are drawn from recent literature and open-source projects in face recognition (2019–2025), including ArcFace ￼ ￼, RetinaFace ￼, SCRFD ￼, AdaFace ￼ ￼, MagFace ￼ ￼, Vision Transformer-based models ￼ ￼, pose-invariant recognition research ￼ ￼, video face recognition aggregation ￼, and scalable retrieval methods Faiss/ScaNN ￼ ￼. These provide a roadmap to build a cutting-edge face recognition system that is accurate, robust, and scalable.
